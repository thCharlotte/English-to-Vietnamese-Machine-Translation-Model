{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T04:02:30.601506Z",
     "iopub.status.busy": "2025-05-30T04:02:30.601205Z",
     "iopub.status.idle": "2025-05-30T04:02:37.686408Z",
     "shell.execute_reply": "2025-05-30T04:02:37.685499Z",
     "shell.execute_reply.started": "2025-05-30T04:02:30.601485Z"
    },
    "id": "ufie_NgK6FKR",
    "outputId": "87906c87-2850-4e2d-852e-e2b290da0269",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=5fc58edfb131dfaa62e4f152ed2fec431211ba88ac69a8ff2b90f74fcc65517e\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: portalocker, fsspec, sacrebleu, rouge-score, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0 portalocker-3.1.1 rouge-score-0.1.2 sacrebleu-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sacrebleu rouge-score evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T04:02:37.688325Z",
     "iopub.status.busy": "2025-05-30T04:02:37.687997Z",
     "iopub.status.idle": "2025-05-30T04:02:37.691800Z",
     "shell.execute_reply": "2025-05-30T04:02:37.691021Z",
     "shell.execute_reply.started": "2025-05-30T04:02:37.688301Z"
    },
    "id": "JHgpm0d7TulK",
    "outputId": "76d0cde0-f440-4ca7-e0b3-331331a43358",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T04:02:37.692768Z",
     "iopub.status.busy": "2025-05-30T04:02:37.692557Z",
     "iopub.status.idle": "2025-05-30T04:03:04.685673Z",
     "shell.execute_reply": "2025-05-30T04:03:04.684890Z",
     "shell.execute_reply.started": "2025-05-30T04:02:37.692752Z"
    },
    "id": "k4nHpP6x6FKV",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 04:02:48.015380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748577768.202569      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748577768.256231      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, json, evaluate, torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MarianTokenizer, MarianMTModel,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq, ProgressCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T04:03:04.687836Z",
     "iopub.status.busy": "2025-05-30T04:03:04.687342Z",
     "iopub.status.idle": "2025-05-30T04:03:04.691352Z",
     "shell.execute_reply": "2025-05-30T04:03:04.690683Z",
     "shell.execute_reply.started": "2025-05-30T04:03:04.687818Z"
    },
    "id": "jAAF26wiFpwk",
    "outputId": "43b65d94-6b79-4483-d63a-d37e1139dd80",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # gg colab\n",
    "# BASE = \"/content/drive/MyDrive/NLP_final/\"\n",
    "# RAW_DIR = os.path.join(BASE, \"data\", \"raw\")\n",
    "# TOKENIZER_DIR = os.path.join(BASE, \"marian_ft\", \"tokenizer\")\n",
    "# OUTPUT = os.path.join(BASE, \"marian_ft\", \"pretrained_mt\")\n",
    "# os.makedirs(OUTPUT, exist_ok=True)\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T04:03:04.692316Z",
     "iopub.status.busy": "2025-05-30T04:03:04.692014Z",
     "iopub.status.idle": "2025-05-30T04:03:04.707654Z",
     "shell.execute_reply": "2025-05-30T04:03:04.706690Z",
     "shell.execute_reply.started": "2025-05-30T04:03:04.692283Z"
    },
    "id": "pLUuwjPM6FKW",
    "outputId": "ba47e877-684a-4c0f-9e75-9f92314e1a9c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# kaggle\n",
    "BASE      = \"/kaggle/input/final-nlp/NLP_final\"\n",
    "RAW_DIR   = os.path.join(BASE, \"data\", \"raw\")        # chứa train.csv, val.csv, test.csv\n",
    "OUTPUT = os.path.join(\"/kaggle/working/marian_ft\", \"pretrained_mt\")     # thư mục lưu model fine-tuned\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T04:03:04.708742Z",
     "iopub.status.busy": "2025-05-30T04:03:04.708492Z",
     "iopub.status.idle": "2025-05-30T04:03:06.155105Z",
     "shell.execute_reply": "2025-05-30T04:03:06.154387Z",
     "shell.execute_reply.started": "2025-05-30T04:03:04.708718Z"
    },
    "id": "D0zQtTfH6FKX",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a4774cf15349058ff7bc4a31542689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d93da68e14466abd7cc9ebc65140a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8022126c3e34dfcbfb81366e52c2c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'vi', 'len_en_words', 'len_vi_words'],\n",
      "        num_rows: 105685\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['en', 'vi', 'len_en_words', 'len_vi_words'],\n",
      "        num_rows: 13210\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'vi', 'len_en_words', 'len_vi_words'],\n",
      "        num_rows: 13211\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {spl : os.path.join(RAW_DIR, f\"{spl}.csv\")\n",
    "              for spl in [\"train\",\"val\",\"test\"]}\n",
    "raw_ds = load_dataset(\"csv\", data_files=data_files, cache_dir=\"/content/cache\")\n",
    "print(raw_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b6db522d1e5e4f77aae3e4e42caf957a",
      "9c19182622614f6a847eaa549aae658e",
      "5c4ad05be9a84c638fb5bd0b8e4ac0b4",
      "badc7f61726e46338c2710bdbcd9b9d3",
      "911a24e311664c7fb4f671b68e344d8d",
      "386289e1f7724fe7b3a37ddddf61be7d",
      "ab3e37dc44e2411480d04a08d667600c",
      "eda2ef16ecbf4e57be5a8da2f99c9869"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T04:03:06.155970Z",
     "iopub.status.busy": "2025-05-30T04:03:06.155791Z",
     "iopub.status.idle": "2025-05-30T04:03:10.288192Z",
     "shell.execute_reply": "2025-05-30T04:03:10.287387Z",
     "shell.execute_reply.started": "2025-05-30T04:03:06.155955Z"
    },
    "id": "y7fdEqZP6FKY",
    "outputId": "ec1a6538-5d23-4f02-dbff-79642e89d04a",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3618ec506064980b3ace42fb302d380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6959ea4e812143458d8b4eab5a89d760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/809k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2f877b139c44639564723a8ce4c895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/756k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3d201169674fe3af98e3da12c1673e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8f91bca5b34d2c8c9f4e5bc2d9c661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3ef3c5108442e1886c3d0488ed9837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/289M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5dcde943a74183bc2e740f5744a4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary = 53685\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"Helsinki-NLP/opus-mt-en-vi\"   # Marian đã fine-tune sẵn\n",
    "tokenizer = MarianTokenizer.from_pretrained(MODEL_ID)\n",
    "model     = MarianMTModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "print(\"Loaded vocabulary =\", tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "0b0b854cc7cd40c3923b7be898aaa547",
      "0ed81d0192204c9fbb48b1b93585dd9b",
      "c189156e6fd842a1b2989725710eddcb"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T04:03:10.289625Z",
     "iopub.status.busy": "2025-05-30T04:03:10.288928Z",
     "iopub.status.idle": "2025-05-30T04:04:01.781925Z",
     "shell.execute_reply": "2025-05-30T04:04:01.781108Z",
     "shell.execute_reply.started": "2025-05-30T04:03:10.289596Z"
    },
    "id": "AcvkK3bq6FKZ",
    "outputId": "4b1b8806-e405-498d-8214-50abca7a66ad",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bf0676e9d047248c0743c48328ba44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/289M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d112986e7694d82854f9b466814e3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenising:   0%|          | 0/105685 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac1e05927b240da9c8c6f2f0bedb394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenising:   0%|          | 0/13210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcee65a7ad84ddd9fa0ea083052a1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenising:   0%|          | 0/13211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask'],\n",
      "        num_rows: 105685\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask'],\n",
      "        num_rows: 13210\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask'],\n",
      "        num_rows: 13211\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN_SRC = 128          # EN\n",
    "MAX_LEN_TGT = 128          # VI\n",
    "\n",
    "def preprocess(batch):\n",
    "    # ---------- encode EN (source) ----------\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"en\"],\n",
    "        max_length=MAX_LEN_SRC,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ---------- encode VI (target) ----------\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"vi\"],\n",
    "            max_length=MAX_LEN_TGT,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    # seq-2-seq Trainer cần:\n",
    "    #   input_ids, attention_mask  – cho encoder\n",
    "    #   labels                    – cho decoder\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# raw_ds là DatasetDict chứa train/val/test\n",
    "tok_ds = raw_ds.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=raw_ds[\"train\"].column_names,   # bỏ cột 'en','vi', ...\n",
    "    desc=\"Tokenising\",\n",
    ")\n",
    "\n",
    "tok_ds.set_format(type=\"torch\")   # tensors ready for DataLoader / Trainer\n",
    "print(tok_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMrAdoVTH951"
   },
   "source": [
    "# Định nghĩa metrics và Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T04:04:01.783112Z",
     "iopub.status.busy": "2025-05-30T04:04:01.782819Z",
     "iopub.status.idle": "2025-05-30T04:04:03.004440Z",
     "shell.execute_reply": "2025-05-30T04:04:03.003862Z",
     "shell.execute_reply.started": "2025-05-30T04:04:01.783087Z"
    },
    "id": "vZ2baWvsH7eH",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544b00d062d4433682d2173e936d9a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd16223ad1948f9ab0f13d32166bcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(results):\n",
    "    preds = tokenizer.batch_decode(results.predictions, skip_special_tokens=True)\n",
    "    refs  = tokenizer.batch_decode(results.label_ids, skip_special_tokens=True)\n",
    "    bleu_s  = bleu.compute(predictions=preds, references=[[r] for r in refs])[\"score\"]\n",
    "    rouge1  = rouge.compute(predictions=preds, references=refs)[\"rouge1\"]*100\n",
    "    rouge2  = rouge.compute(predictions=preds, references=refs)[\"rouge2\"]*100\n",
    "    rougel  = rouge.compute(predictions=preds, references=refs)[\"rougeL\"]*100\n",
    "    return {\"bleu\": bleu_s, \"rouge1\": rouge1, \"rouge2\": rouge2, \"rougeL\": rougel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "2aebcd0ecaea4c88a153b69ebaf60789",
      "42b41dabe717425c8bd8ef1a6807f721"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T04:04:03.006845Z",
     "iopub.status.busy": "2025-05-30T04:04:03.006378Z",
     "iopub.status.idle": "2025-05-30T04:04:03.066785Z",
     "shell.execute_reply": "2025-05-30T04:04:03.066111Z",
     "shell.execute_reply.started": "2025-05-30T04:04:03.006827Z"
    },
    "id": "fe2gHBuP6FKa",
    "outputId": "487e0e20-dc12-4b59-930c-94c6a1667594",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3823456764.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "BATCH = 8\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir          = OUTPUT,\n",
    "    per_device_train_batch_size = BATCH,\n",
    "    per_device_eval_batch_size  = BATCH,\n",
    "    learning_rate       = 5e-5,\n",
    "    num_train_epochs    = 3,\n",
    "    predict_with_generate = True,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy       = \"epoch\",\n",
    "    logging_steps       = 1000,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model  = \"bleu\",\n",
    "    greater_is_better      = True,\n",
    "    fp16 = torch.cuda.is_available(),\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model           = model,\n",
    "    args            = args,\n",
    "    train_dataset   = tok_ds[\"train\"],\n",
    "    eval_dataset    = tok_ds[\"val\"],\n",
    "    tokenizer       = tokenizer,\n",
    "    data_collator   = DataCollatorForSeq2Seq(tokenizer, model),\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks       = [ProgressCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "bb994efd136e41d7ad0211d20b66d09a",
      ""
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T04:04:03.068175Z",
     "iopub.status.busy": "2025-05-30T04:04:03.067574Z",
     "iopub.status.idle": "2025-05-30T06:07:14.990718Z",
     "shell.execute_reply": "2025-05-30T06:07:14.990122Z",
     "shell.execute_reply.started": "2025-05-30T04:04:03.068157Z"
    },
    "id": "2fGlm-9S6FKb",
    "outputId": "2cdd21dc-59a2-4461-d035-c4f7aef24895",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d045c26605744fd1bc49a4d8e903ccb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19818' max='19818' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19818/19818 2:03:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.292178</td>\n",
       "      <td>33.907576</td>\n",
       "      <td>74.154676</td>\n",
       "      <td>53.322948</td>\n",
       "      <td>66.764912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.288144</td>\n",
       "      <td>34.596238</td>\n",
       "      <td>74.256556</td>\n",
       "      <td>53.578936</td>\n",
       "      <td>66.953541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>0.287992</td>\n",
       "      <td>34.681965</td>\n",
       "      <td>74.344863</td>\n",
       "      <td>53.681072</td>\n",
       "      <td>67.078207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3515, 'grad_norm': 52285.3515625, 'learning_rate': 4.747956403269755e-05, 'epoch': 0.15}\n",
      "{'loss': 0.3251, 'grad_norm': 57439.95703125, 'learning_rate': 4.4956605106468866e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3206, 'grad_norm': 44365.2421875, 'learning_rate': 4.243364618024019e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3216, 'grad_norm': 42953.203125, 'learning_rate': 3.991068725401151e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3194, 'grad_norm': 43031.09375, 'learning_rate': 3.738772832778282e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3154, 'grad_norm': 61441.70703125, 'learning_rate': 3.486476940155414e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29217833280563354, 'eval_bleu': 33.90757598944195, 'eval_rouge1': 74.1546759190035, 'eval_rouge2': 53.32294794814618, 'eval_rougeL': 66.76491172911597, 'eval_runtime': 853.3356, 'eval_samples_per_second': 15.48, 'eval_steps_per_second': 0.968, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3031, 'grad_norm': 35308.015625, 'learning_rate': 3.2341810475325464e-05, 'epoch': 1.06}\n",
      "{'loss': 0.2791, 'grad_norm': 45053.62109375, 'learning_rate': 2.981885154909678e-05, 'epoch': 1.21}\n",
      "{'loss': 0.282, 'grad_norm': 57616.20703125, 'learning_rate': 2.7295892622868102e-05, 'epoch': 1.36}\n",
      "{'loss': 0.278, 'grad_norm': 52661.1640625, 'learning_rate': 2.477293369663942e-05, 'epoch': 1.51}\n",
      "{'loss': 0.2809, 'grad_norm': 44867.98828125, 'learning_rate': 2.224997477041074e-05, 'epoch': 1.67}\n",
      "{'loss': 0.2787, 'grad_norm': 42851.1796875, 'learning_rate': 1.9727015844182055e-05, 'epoch': 1.82}\n",
      "{'loss': 0.2775, 'grad_norm': 48448.62890625, 'learning_rate': 1.7204056917953376e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28814366459846497, 'eval_bleu': 34.59623795771896, 'eval_rouge1': 74.25655622289409, 'eval_rouge2': 53.57893615469026, 'eval_rougeL': 66.9535405988039, 'eval_runtime': 820.7666, 'eval_samples_per_second': 16.095, 'eval_steps_per_second': 1.006, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2607, 'grad_norm': 37375.6328125, 'learning_rate': 1.4681097991724697e-05, 'epoch': 2.12}\n",
      "{'loss': 0.252, 'grad_norm': 40247.55859375, 'learning_rate': 1.2158139065496014e-05, 'epoch': 2.27}\n",
      "{'loss': 0.2547, 'grad_norm': 43930.07421875, 'learning_rate': 9.635180139267333e-06, 'epoch': 2.42}\n",
      "{'loss': 0.2512, 'grad_norm': 48895.39453125, 'learning_rate': 7.112221213038652e-06, 'epoch': 2.57}\n",
      "{'loss': 0.2542, 'grad_norm': 40019.375, 'learning_rate': 4.589262286809971e-06, 'epoch': 2.72}\n",
      "{'loss': 0.2544, 'grad_norm': 36068.859375, 'learning_rate': 2.06630336058129e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28799161314964294, 'eval_bleu': 34.681964977363904, 'eval_rouge1': 74.34486258591357, 'eval_rouge2': 53.681071754599195, 'eval_rougeL': 67.07820727192582, 'eval_runtime': 821.1559, 'eval_samples_per_second': 16.087, 'eval_steps_per_second': 1.006, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7390.7384, 'train_samples_per_second': 42.899, 'train_steps_per_second': 2.681, 'train_loss': 0.28603565686886406, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/marian_ft/pretrained_mt/tokenizer_config.json',\n",
       " '/kaggle/working/marian_ft/pretrained_mt/special_tokens_map.json',\n",
       " '/kaggle/working/marian_ft/pretrained_mt/vocab.json',\n",
       " '/kaggle/working/marian_ft/pretrained_mt/source.spm',\n",
       " '/kaggle/working/marian_ft/pretrained_mt/target.spm',\n",
       " '/kaggle/working/marian_ft/pretrained_mt/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(OUTPUT)\n",
    "tokenizer.save_pretrained(OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:07:14.991610Z",
     "iopub.status.busy": "2025-05-30T06:07:14.991429Z",
     "iopub.status.idle": "2025-05-30T06:07:14.997792Z",
     "shell.execute_reply": "2025-05-30T06:07:14.997284Z",
     "shell.execute_reply.started": "2025-05-30T06:07:14.991595Z"
    },
    "id": "UW0fgmsk6FKb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH = 8\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tok_ds[\"val\"].with_format(\"torch\"),\n",
    "    batch_size = BATCH,\n",
    "    shuffle    = False,\n",
    ")\n",
    "test_loader = DataLoader(tok_ds[\"test\"].with_format(\"torch\"), batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "1ff36ce1628b48cfa87f72ee119bfc77"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-30T06:07:14.998631Z",
     "iopub.status.busy": "2025-05-30T06:07:14.998468Z",
     "iopub.status.idle": "2025-05-30T06:19:52.817727Z",
     "shell.execute_reply": "2025-05-30T06:19:52.817109Z",
     "shell.execute_reply.started": "2025-05-30T06:07:14.998619Z"
    },
    "id": "OOv7dQIb6FKd",
    "outputId": "90923dfe-5067-4396-985f-d3e0e6280be1",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22be45db1e824c019c031ea702d4ea34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/1652 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU      : 34.39\n",
      "ROUGE-1   : 73.88\n",
      "ROUGE-2   : 52.88\n",
      "ROUGE-L   : 66.56\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def marian_translate_batch(loader, model, tokenizer, device, num_beams=1, max_len=128):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for batch in tqdm(loader, desc=\"Translating\", unit=\"batch\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=max_len,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        preds.extend(tokenizer.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        labs = batch[\"labels\"].clone()\n",
    "        labs[labs == -100] = tokenizer.pad_token_id\n",
    "        refs.extend(tokenizer.batch_decode(labs, skip_special_tokens=True))\n",
    "    return preds, refs\n",
    "\n",
    "preds, refs = marian_translate_batch(test_loader, model, tokenizer, DEVICE, num_beams=1, max_len=128)\n",
    "\n",
    "# In điểm BLEU và ROUGE-1/2/L\n",
    "bleu_score = bleu.compute(predictions=preds, references=[[r] for r in refs])['score']\n",
    "rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "\n",
    "print(\"BLEU      :\", round(bleu_score, 2))\n",
    "print(\"ROUGE-1   :\", round(rouge_scores['rouge1']*100, 2))\n",
    "print(\"ROUGE-2   :\", round(rouge_scores['rouge2']*100, 2))\n",
    "print(\"ROUGE-L   :\", round(rouge_scores['rougeL']*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:22:02.757231Z",
     "iopub.status.busy": "2025-05-30T06:22:02.756851Z",
     "iopub.status.idle": "2025-05-30T06:22:02.763829Z",
     "shell.execute_reply": "2025-05-30T06:22:02.763030Z",
     "shell.execute_reply.started": "2025-05-30T06:22:02.757209Z"
    },
    "id": "JKHCEMAs6FKe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def marian_translate(texts,\n",
    "                     model=model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     device=DEVICE,\n",
    "                     num_beams=1,\n",
    "                     max_len=128,\n",
    "                     clean=True):\n",
    "    \"\"\"\n",
    "    texts   : str  | list[str] – câu/ các câu Tiếng Anh\n",
    "    returns : str  | list[str] – câu/ các câu Tiếng Việt\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        single = True\n",
    "        texts  = [texts]\n",
    "    else:\n",
    "        single = False\n",
    "\n",
    "    # 1) Tokenize hàng loạt -> chuyển sang device\n",
    "    batch = tokenizer(texts,\n",
    "                      return_tensors=\"pt\",\n",
    "                      padding=True,\n",
    "                      truncation=True,\n",
    "                      max_length=max_len).to(device)\n",
    "\n",
    "    # 2) Generate\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            input_ids      = batch[\"input_ids\"],\n",
    "            attention_mask = batch[\"attention_mask\"],\n",
    "            max_length     = max_len,\n",
    "            num_beams      = num_beams,\n",
    "            early_stopping = True\n",
    "        )\n",
    "\n",
    "    # 3) Decode\n",
    "    outs = tokenizer.batch_decode(gen_ids,\n",
    "                                  skip_special_tokens=True)\n",
    "\n",
    "    # 4) (optional) làm sạch dấu cách trước dấu câu\n",
    "    if clean:\n",
    "        outs = [s.replace(\" .\", \".\")\n",
    "                  .replace(\" ,\", \",\")\n",
    "                  .replace(\" !\", \"!\")\n",
    "                  .replace(\" ?\", \"?\")\n",
    "                for s in outs]\n",
    "\n",
    "    return outs[0] if single else outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:22:05.509214Z",
     "iopub.status.busy": "2025-05-30T06:22:05.508865Z",
     "iopub.status.idle": "2025-05-30T06:22:05.840838Z",
     "shell.execute_reply": "2025-05-30T06:22:05.840080Z",
     "shell.execute_reply.started": "2025-05-30T06:22:05.509193Z"
    },
    "id": "k5-opgNv6FKf",
    "outputId": "0b817df3-a288-4729-c484-27969ac90df3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Translations ---\n",
      "EN: I want to go to sleep\n",
      "VI: Tôi muốn đi ngủ.\n",
      "\n",
      "EN: Good morning! Did you sleep well?\n",
      "VI: Chúc buổi sáng tốt lành! Bạn có ngủ ngon không?\n",
      "\n",
      "EN: Yesterday the stock market plunged almost five percent.\n",
      "VI: Ngày hôm qua thị trường chứng khoán đã giảm gần 5 %.\n",
      "\n",
      "EN: She wonders whether artificial intelligence will ever surpass human creativity.\n",
      "VI: Cô tự hỏi liệu trí tuệ nhân tạo có vượt qua được sự sáng tạo của con người hay không.\n",
      "\n",
      "EN: Please, turn off the lights before you leave the laboratory.\n",
      "VI: Làm ơn tắt đèn đi trước khi bạn rời phòng thí nghiệm.\n",
      "\n",
      "EN: Although it rained heavily, the concert continued until midnight.\n",
      "VI: Mặc dù trời mưa nhiều, buổi hoà nhạc vẫn tiếp tục cho đến nửa đêm.\n",
      "\n",
      "EN: The report, which was published in 2023, estimates that global CO₂ emissions hit 37.4 gigatons.\n",
      "VI: Báo cáo, xuất bản năm 2023, ước tính lượng lượng khí thải CO2 toàn cầu đã giảm 37.4 tỉ tấn.\n",
      "\n",
      "EN: Have you ever tried Vietnamese egg coffee?\n",
      "VI: Bạn đã bao giờ thử cà phê trứng Việt Nam chưa?\n",
      "\n",
      "EN: If we fail to act now, future generations will pay the price.\n",
      "VI: Nếu chúng ta không hành động ngay bây giờ, thế hệ tương lai sẽ phải trả giá.\n",
      "\n",
      "EN: NASA's James Webb telescope recently captured breathtaking images of distant galaxies.\n",
      "VI: Kính viễn vọng James Webb của NASA gần đây đã chụp được những hình ảnh ngoạn mục về những thiên hà xa xôi.\n",
      "\n",
      "EN: In my opinion, learning a new language is like opening an extra window on the world.\n",
      "VI: Theo tôi, học một ngôn ngữ mới giống như mở thêm một cửa sổ trên thế giới.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Nhiều câu\n",
    "en_batch = [\n",
    "    \"I want to go to sleep\",\n",
    "     \"Good morning! Did you sleep well?\",\n",
    "    \"Yesterday the stock market plunged almost five percent.\",\n",
    "    \"She wonders whether artificial intelligence will ever surpass human creativity.\",\n",
    "    \"Please, turn off the lights before you leave the laboratory.\",\n",
    "    \"Although it rained heavily, the concert continued until midnight.\",\n",
    "    \"The report, which was published in 2023, estimates that global CO₂ emissions hit 37.4 gigatons.\",\n",
    "    \"Have you ever tried Vietnamese egg coffee?\",\n",
    "    \"If we fail to act now, future generations will pay the price.\",\n",
    "    \"NASA's James Webb telescope recently captured breathtaking images of distant galaxies.\",\n",
    "    \"In my opinion, learning a new language is like opening an extra window on the world.\"\n",
    "]\n",
    "vi_batch = marian_translate(en_batch, num_beams=5)\n",
    "print(\"\\n--- Sample Translations ---\")\n",
    "for en, vi in zip(en_batch, vi_batch):\n",
    "    print(f\"EN: {en}\")\n",
    "    print(f\"VI: {vi}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:19:53.160515Z",
     "iopub.status.busy": "2025-05-30T06:19:53.160266Z",
     "iopub.status.idle": "2025-05-30T06:19:53.163769Z",
     "shell.execute_reply": "2025-05-30T06:19:53.163099Z",
     "shell.execute_reply.started": "2025-05-30T06:19:53.160499Z"
    },
    "id": "DiZxmyOkSEZO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# zip_path = os.path.join(BASE, \"marian_ft_deploy.zip\")\n",
    "# shutil.make_archive(zip_path.replace('.zip', ''), \"zip\", OUTPUT)\n",
    "# print(f\"Đã tạo file zip model cho deploy: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:19:53.164821Z",
     "iopub.status.busy": "2025-05-30T06:19:53.164586Z",
     "iopub.status.idle": "2025-05-30T06:19:53.174461Z",
     "shell.execute_reply": "2025-05-30T06:19:53.173790Z",
     "shell.execute_reply.started": "2025-05-30T06:19:53.164799Z"
    },
    "id": "IBgy3sjL6FKf",
    "outputId": "db387766-bea2-4da8-d87e-d30a318773b9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# # ===== Cell 1: zip inference package =====\n",
    "# EXP_DIR=/kaggle/working/marian_ft          # ← đổi nếu khác\n",
    "# PKG_NAME=marian_ft_infer.zip               # tên file zip sẽ tạo\n",
    "\n",
    "# cd \"$EXP_DIR\"/..\n",
    "\n",
    "# zip -r \"$PKG_NAME\" \\\n",
    "#     marian_ft/model.safetensors \\\n",
    "#     marian_ft/config.json \\\n",
    "#     marian_ft/generation_config.json \\\n",
    "#     marian_ft/source.spm marian_ft/target.spm \\\n",
    "#     marian_ft/vocab.json \\\n",
    "#     marian_ft/tokenizer_config.json \\\n",
    "#     marian_ft/special_tokens_map.json\n",
    "\n",
    "# echo \"✅  Đã tạo: $(pwd)/$PKG_NAME\"\n",
    "# ls -lh \"$PKG_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:19:53.175476Z",
     "iopub.status.busy": "2025-05-30T06:19:53.175241Z",
     "iopub.status.idle": "2025-05-30T06:19:53.184860Z",
     "shell.execute_reply": "2025-05-30T06:19:53.184280Z",
     "shell.execute_reply.started": "2025-05-30T06:19:53.175454Z"
    },
    "id": "hVxWCVek6FKg",
    "outputId": "c15a1745-0d91-4622-97fd-47fd3ebc77d0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ==== Cell 3: Load lại ZIP của MarianFT (inference) và test bản dịch ====\n",
    "# import os, zipfile, tempfile\n",
    "# import torch\n",
    "# from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "# # 1) Đường dẫn tới file ZIP bạn đã tạo (trong /kaggle/working)\n",
    "# ZIP_PATH = \"/kaggle/working/marian_ft_infer.zip\"\n",
    "\n",
    "# # 2) Giải nén vào thư mục tạm\n",
    "# tmpdir = tempfile.mkdtemp()\n",
    "# with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "#     z.extractall(tmpdir)\n",
    "\n",
    "# # 3) Tự động tìm sub-folder chứa config.json/tokenizer files\n",
    "# model_dir = None\n",
    "# for root, dirs, files in os.walk(tmpdir):\n",
    "#     if \"config.json\" in files and \"tokenizer_config.json\" in files:\n",
    "#         model_dir = root\n",
    "#         break\n",
    "# if model_dir is None:\n",
    "#     model_dir = tmpdir  # fallback\n",
    "\n",
    "# print(f\"→ Loading model/tokenizer from: {model_dir}\")\n",
    "\n",
    "# # 4) Load tokenizer + model\n",
    "# tokenizer = MarianTokenizer.from_pretrained(model_dir)\n",
    "# model     = MarianMTModel.from_pretrained(model_dir).to(\"cpu\")  # or \"cuda\"\n",
    "\n",
    "# # 5) Test dịch sample\n",
    "# sent = \"Hello, how are you?\"\n",
    "# inputs  = tokenizer(sent, return_tensors=\"pt\")\n",
    "# outputs = model.generate(**inputs, max_length=64)\n",
    "# print(\"EN:\", sent)\n",
    "# print(\"VI:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7504992,
     "sourceId": 11937288,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
