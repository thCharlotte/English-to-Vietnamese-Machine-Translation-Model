{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T23:51:16.169062Z",
     "iopub.status.busy": "2025-05-26T23:51:16.168803Z",
     "iopub.status.idle": "2025-05-26T23:51:23.856959Z",
     "shell.execute_reply": "2025-05-26T23:51:23.856154Z",
     "shell.execute_reply.started": "2025-05-26T23:51:16.169035Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: rouge, portalocker, fsspec, sacrebleu, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0 portalocker-3.1.1 rouge-1.0.1 sacrebleu-2.5.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Install Dependencies\n",
    "!pip install rouge evaluate transformers sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T23:51:36.323559Z",
     "iopub.status.busy": "2025-05-26T23:51:36.323034Z",
     "iopub.status.idle": "2025-05-26T23:52:16.907726Z",
     "shell.execute_reply": "2025-05-26T23:52:16.907161Z",
     "shell.execute_reply.started": "2025-05-26T23:51:36.323528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:51:58.780032: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748303519.241030      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748303519.363324      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast, GPT2Config, GPT2LMHeadModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# Thiết bị\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper‐params\n",
    "INPUT_DIR      = '/kaggle/input/final-nlp/NLP_final/data/raw'\n",
    "TOKENIZER_PATH = '/kaggle/input/final-nlp/NLP_final/tokenizer_bpe/model/tokenizer.json'\n",
    "CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE           = 4\n",
    "ACCUMULATION_STEPS   = 2      # để effective batch size = 8\n",
    "MAX_LENGTH           = 128\n",
    "NUM_EPOCHS           = 5\n",
    "LEARNING_RATE        = 5e-5\n",
    "WEIGHT_DECAY         = 0.01\n",
    "WARMUP_STEPS         = 500\n",
    "GRAD_CLIP_NORM       = 1.0\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=TOKENIZER_PATH,\n",
    "    bos_token='<s>', eos_token='</s>',\n",
    "    unk_token='<unk>', pad_token='<pad>'\n",
    ")\n",
    "\n",
    "# Sample sentences để inference nhanh\n",
    "sample_en = [\n",
    "    \"I want to go to sleep\",\n",
    "    \"Good morning! Did you sleep well?\",\n",
    "    \"Yesterday the stock market plunged almost five percent.\",\n",
    "    \"She wonders whether artificial intelligence will ever surpass human creativity.\",\n",
    "    \"Please, turn off the lights before you leave the laboratory.\",\n",
    "    \"Although it rained heavily, the concert continued until midnight.\",\n",
    "    \"The report, which was published in 2023, estimates that global CO₂ emissions hit 37.4 gigatons.\",\n",
    "    \"Have you ever tried Vietnamese egg coffee?\",\n",
    "    \"If we fail to act now, future generations will pay the price.\",\n",
    "    \"NASA's James Webb telescope recently captured breathtaking images of distant galaxies.\",\n",
    "    \"In my opinion, learning a new language is like opening an extra window on the world.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T23:55:03.892537Z",
     "iopub.status.busy": "2025-05-26T23:55:03.891547Z",
     "iopub.status.idle": "2025-05-26T23:55:05.120978Z",
     "shell.execute_reply": "2025-05-26T23:55:05.120358Z",
     "shell.execute_reply.started": "2025-05-26T23:55:03.892511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, path_csv, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.df = pd.read_csv(path_csv)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.df.iloc[idx]['en']\n",
    "        tgt = self.df.iloc[idx]['vi']\n",
    "        prompt = f\"Translate English to Vietnamese: {src} {self.tokenizer.eos_token}\"\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            prompt, return_tensors='pt',\n",
    "            truncation=True, max_length=self.max_length//2\n",
    "        )\n",
    "        dec = self.tokenizer(\n",
    "            tgt + self.tokenizer.eos_token, return_tensors='pt',\n",
    "            truncation=True, max_length=self.max_length//2\n",
    "        )\n",
    "\n",
    "        input_ids      = torch.cat([enc.input_ids[0],      dec.input_ids[0]], dim=0)\n",
    "        attention_mask = torch.cat([enc.attention_mask[0], dec.attention_mask[0]], dim=0)\n",
    "        labels         = input_ids.clone()\n",
    "        labels[:enc.input_ids.size(1)] = -100\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids      = pad_sequence([b['input_ids']      for b in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([b['attention_mask'] for b in batch], batch_first=True, padding_value=0)\n",
    "    labels         = pad_sequence([b['labels']         for b in batch], batch_first=True, padding_value=-100)\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "train_ds = TranslationDataset(os.path.join(INPUT_DIR,'train.csv'), tokenizer)\n",
    "val_ds   = TranslationDataset(os.path.join(INPUT_DIR,'val.csv'),   tokenizer)\n",
    "test_ds  = TranslationDataset(os.path.join(INPUT_DIR,'test.csv'),  tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T23:55:21.595416Z",
     "iopub.status.busy": "2025-05-26T23:55:21.594815Z",
     "iopub.status.idle": "2025-05-26T23:55:23.621512Z",
     "shell.execute_reply": "2025-05-26T23:55:23.620680Z",
     "shell.execute_reply.started": "2025-05-26T23:55:21.595392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=MAX_LENGTH,\n",
    "    n_ctx=MAX_LENGTH,\n",
    "    n_embd=768,\n",
    "    n_layer=8,\n",
    "    n_head=12,\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1\n",
    ")\n",
    "model = GPT2LMHeadModel(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T19:52:27.708860Z",
     "iopub.status.busy": "2025-05-26T19:52:27.708241Z",
     "iopub.status.idle": "2025-05-26T22:56:28.715023Z",
     "shell.execute_reply": "2025-05-26T22:56:28.713815Z",
     "shell.execute_reply.started": "2025-05-26T19:52:27.708835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65d22a2e7834c85b00163951cf813a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/26422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 4.2738 — Val Loss: 3.4259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea1d6d20c0e4fafb8ac2173d42b5ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/26422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 3.0764 — Val Loss: 2.7779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1410b0e36c47cf89000853b2b9292f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/26422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 2.5261 — Val Loss: 2.5182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077e4c8e5c0d4cbc9ddf7d522301176e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/26422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 2.1919 — Val Loss: 2.3372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e05ccec99943608e2e8c8a8287b099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/26422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 1.9656 — Val Loss: 2.2850\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "total_steps = len(train_loader) // ACCUMULATION_STEPS * NUM_EPOCHS\n",
    "scheduler   = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "best_val_loss = float('inf')\n",
    "stale = 0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}\")):\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            labels=batch['labels'].to(device)\n",
    "        )\n",
    "        loss = outputs.loss / ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if (step+1) % ACCUMULATION_STEPS == 0:\n",
    "            clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    avg_train = train_loss * ACCUMULATION_STEPS / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                labels=batch['labels'].to(device)\n",
    "            )\n",
    "            val_loss += out.loss.item()\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_train:.4f} — Val Loss: {avg_val:.4f}\")\n",
    "\n",
    "    # Checkpoint & Early Stop\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR,'best_model.pt'))\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale >= 2:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T23:24:48.515841Z",
     "iopub.status.busy": "2025-05-26T23:24:48.515569Z",
     "iopub.status.idle": "2025-05-26T23:25:05.914282Z",
     "shell.execute_reply": "2025-05-26T23:25:05.913519Z",
     "shell.execute_reply.started": "2025-05-26T23:24:48.515821Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/checkpoints/ (stored 0%)\n",
      "  adding: kaggle/working/checkpoints/best_model.pt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (deflated 7%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Zip Model & Checkpoints\n",
    "!zip -r /kaggle/working/model_checkpoint.zip /kaggle/working/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T23:25:32.948806Z",
     "iopub.status.busy": "2025-05-26T23:25:32.948098Z",
     "iopub.status.idle": "2025-05-26T23:25:49.482365Z",
     "shell.execute_reply": "2025-05-26T23:25:49.481483Z",
     "shell.execute_reply.started": "2025-05-26T23:25:32.948777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo /kaggle/working/model_checkpoint.zip\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Zip Model & Checkpoints (Python)\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('/kaggle/working/model_checkpoint', 'zip', '/kaggle/working/checkpoints')\n",
    "print(\"Đã tạo /kaggle/working/model_checkpoint.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T23:56:43.363082Z",
     "iopub.status.busy": "2025-05-26T23:56:43.362703Z",
     "iopub.status.idle": "2025-05-26T23:56:46.793681Z",
     "shell.execute_reply": "2025-05-26T23:56:46.793181Z",
     "shell.execute_reply.started": "2025-05-26T23:56:43.363058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111a7a04b72c4d90a05da9e59ff7aadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load best\n",
    "model.load_state_dict(torch.load(os.path.join(\"/kaggle/input/model-cpkt\",'best_model.pt')))\n",
    "model.eval()\n",
    "bleu = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T23:56:56.974185Z",
     "iopub.status.busy": "2025-05-26T23:56:56.973505Z",
     "iopub.status.idle": "2025-05-26T23:56:56.979930Z",
     "shell.execute_reply": "2025-05-26T23:56:56.979074Z",
     "shell.execute_reply.started": "2025-05-26T23:56:56.974159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate(text, num_beams=1, max_new_tokens=64):\n",
    "    # 1) Tạo prompt\n",
    "    prompt = f\"Translate English to Vietnamese: {text} {tokenizer.eos_token}\"\n",
    "    # 2) Tokenize & truncate\n",
    "    encoding = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH//2\n",
    "    )\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "\n",
    "    # 3) Generate without attention_mask\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=(num_beams>1),\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        length_penalty=1.0,\n",
    "        use_cache=True\n",
    "    )\n",
    "    # 4) Lấy phần mới sinh\n",
    "    return tokenizer.decode(out[0, input_ids.size(-1):], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T23:57:04.155647Z",
     "iopub.status.busy": "2025-05-26T23:57:04.155368Z",
     "iopub.status.idle": "2025-05-27T00:35:06.063460Z",
     "shell.execute_reply": "2025-05-27T00:35:06.062654Z",
     "shell.execute_reply.started": "2025-05-26T23:57:04.155630Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee18dc905c5143bf87d8ea6f65c839ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Greedy Translation:   0%|          | 0/13210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy BLEU: 23.212151785809066\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Giả sử val_ds.df là DataFrame validation với cột 'en' và 'vi'\n",
    "refs = [[v] for v in val_ds.df['vi']]\n",
    "\n",
    "# Greedy translation với thanh tiến độ\n",
    "preds_greedy = []\n",
    "for src in tqdm(val_ds.df['en'], desc=\"Greedy Translation\"):\n",
    "    preds_greedy.append(translate(src, num_beams=1))\n",
    "\n",
    "# Beam-5 translation với thanh tiến độ\n",
    "# preds_beam5 = []\n",
    "# for src in tqdm(val_ds.df['en'], desc=\"Beam-5 Translation\"):\n",
    "#     preds_beam5.append(translate(src, num_beams=5))\n",
    "\n",
    "# Tính BLEU\n",
    "print(\"Greedy BLEU:\", bleu.compute(predictions=preds_greedy, references=refs)[\"score\"])\n",
    "# print(\"Beam-5 BLEU:\", bleu.compute(predictions=preds_beam5,  references=refs)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "refs = [[v] for v in val_ds.df['vi']]\n",
    "preds_greedy = [translate(s, num_beams=1) for s in val_ds.df['en']]\n",
    "#preds_beam5  = [translate(s, num_beams=5) for s in val_ds.df['en']]\n",
    "\n",
    "print(\"Greedy BLEU:\", bleu.compute(predictions=preds_greedy, references=refs)['score'])\n",
    "# print(\"Beam-5 BLEU:\", bleu.compute(predictions=preds_beam5,  references=refs)['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:37:19.455582Z",
     "iopub.status.busy": "2025-05-27T00:37:19.454821Z",
     "iopub.status.idle": "2025-05-27T01:31:11.918048Z",
     "shell.execute_reply": "2025-05-27T01:31:11.917393Z",
     "shell.execute_reply.started": "2025-05-27T00:37:19.455557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0337feb95624effbc86f9f105a238e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam-5 Translation:   0%|          | 0/13210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam-5 BLEU: 24.621177714770397\n"
     ]
    }
   ],
   "source": [
    "preds_beam5 = []\n",
    "for src in tqdm(val_ds.df['en'], desc=\"Beam-5 Translation\"):\n",
    "    preds_beam5.append(translate(src, num_beams=5))\n",
    "print(\"Beam-5 BLEU:\", bleu.compute(predictions=preds_beam5,  references=refs)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:56:18.336126Z",
     "iopub.status.busy": "2025-05-27T01:56:18.335792Z",
     "iopub.status.idle": "2025-05-27T01:56:36.908630Z",
     "shell.execute_reply": "2025-05-27T01:56:36.907968Z",
     "shell.execute_reply.started": "2025-05-27T01:56:18.336098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy ROUGE-1   : 67.72\n",
      "Greedy ROUGE-2   : 43.2\n",
      "Greedy ROUGE-L   : 57.99\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = evaluate.load('rouge')\n",
    "flat_refs = [r[0] for r in refs]\n",
    "\n",
    "rouge_scores = rouge_metric.compute(\n",
    "    predictions=preds_greedy,\n",
    "    references=flat_refs,\n",
    "    use_stemmer=True         \n",
    ")\n",
    "\n",
    "print(\"Greedy ROUGE-1   :\", round(rouge_scores['rouge1'] * 100, 2))\n",
    "print(\"Greedy ROUGE-2   :\", round(rouge_scores['rouge2'] * 100, 2))\n",
    "print(\"Greedy ROUGE-L   :\", round(rouge_scores['rougeL'] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:56:40.119699Z",
     "iopub.status.busy": "2025-05-27T01:56:40.119320Z",
     "iopub.status.idle": "2025-05-27T01:56:57.748183Z",
     "shell.execute_reply": "2025-05-27T01:56:57.747528Z",
     "shell.execute_reply.started": "2025-05-27T01:56:40.119659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam-5 ROUGE-1  : 68.46\n",
      "Beam-5 ROUGE-2  : 44.43\n",
      "Beam-5 ROUGE-L  : 59.03\n"
     ]
    }
   ],
   "source": [
    "rouge_beam5 = rouge_metric.compute(\n",
    "    predictions=preds_beam5,\n",
    "    references=flat_refs,\n",
    "    use_stemmer=True\n",
    ")\n",
    "print(\"Beam-5 ROUGE-1  :\", round(rouge_beam5['rouge1'] * 100, 2))\n",
    "print(\"Beam-5 ROUGE-2  :\", round(rouge_beam5['rouge2'] * 100, 2))\n",
    "print(\"Beam-5 ROUGE-L  :\", round(rouge_beam5['rougeL'] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:31:54.732443Z",
     "iopub.status.busy": "2025-05-27T01:31:54.732160Z",
     "iopub.status.idle": "2025-05-27T01:31:57.908466Z",
     "shell.execute_reply": "2025-05-27T01:31:57.907869Z",
     "shell.execute_reply.started": "2025-05-27T01:31:54.732411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Translations ---\n",
      "\n",
      ">> I want to go to sleep\n",
      "  - Greedy: Tôi muốn ngủ ngủ .\n",
      "  - Beam-5: Tôi muốn đi ngủ .\n",
      "\n",
      ">> Good morning! Did you sleep well?\n",
      "  - Greedy: Xin chào buổi sáng bạn ngủ tốt bạn ngủ .\n",
      "  - Beam-5: Xin chào các bạn cũng có thể ngủ ngon .\n",
      "\n",
      ">> Yesterday the stock market plunged almost five percent.\n",
      "  - Greedy: Những thị trường chứng khoán của thị trường tiểu bang gần 5 %\n",
      "  - Beam-5: Ngày hôm qua thị trường chứng khoán có 5 %\n",
      "\n",
      ">> She wonders whether artificial intelligence will ever surpass human creativity.\n",
      "  - Greedy: Cô ấy sẽ cho rằng trí thông minh nhân tạo nhân tạo sẽ vượt qua sự sáng tạo con người .\n",
      "  - Beam-5: Cô ấy cho rằng liệu trí thông minh nhân tạo nhân tạo có thể vượt qua sự sáng tạo con người .\n",
      "\n",
      ">> Please, turn off the lights before you leave the laboratory.\n",
      "  - Greedy: Xin hãy tắt đèn trước khi bạn rời phòng thí nghiệm\n",
      "  - Beam-5: Hãy tắt đèn trước khi bạn rời phòng thí nghiệm\n",
      "\n",
      ">> Although it rained heavily, the concert continued until midnight.\n",
      "  - Greedy: Mặc dù nó đã được sử dụng nhiều hơn.000 buổi hoà nhạc cho đến đêm .\n",
      "  - Beam-5: Mặc dù nó đã tồn tại nhiều hơn.000 buổi hoà nhạc cho đến nửa đêm .\n",
      "\n",
      ">> The report, which was published in 2023, estimates that global CO₂ emissions hit 37.4 gigatons.\n",
      "  - Greedy: Bài báo cáo được xuất bản trong 20.000 ước tính toàn cầu mà khí thải khí thải toàn cầu đã bị phá huỷ vào năm 37.7444,7\n",
      "  - Beam-5: Các báo cáo được xuất bản trong 20,000 ước tính toàn cầu rằng khí thải khí thải toàn cầu đã bị phá vỡ tới 37.7,4,7,7\n",
      "\n",
      ">> Have you ever tried Vietnamese egg coffee?\n",
      "  - Greedy: Bạn đã bao giờ thử lấy cà phê trứng\n",
      "  - Beam-5: Bạn đã bao giờ thử lấy cà phê trứng .\n",
      "\n",
      ">> If we fail to act now, future generations will pay the price.\n",
      "  - Greedy: Nếu chúng ta thất bại trong tương lai sẽ trả giá của tương lai .\n",
      "  - Beam-5: Nếu chúng ta thất bại trong tương lai tương lai sẽ trả giá .\n",
      "\n",
      ">> NASA's James Webb telescope recently captured breathtaking images of distant galaxies.\n",
      "  - Greedy: Nhà nghiên cứu của NASA , James Hawkins đã chụp những hình ảnh đáng sợ của những thiên hà xa xôi .\n",
      "  - Beam-5: Nhà thám hiểm của NASA có thể chụp lại những hình ảnh khác nhau của những thiên hà xa xôi .\n",
      "\n",
      ">> In my opinion, learning a new language is like opening an extra window on the world.\n",
      "  - Greedy: Theo tôi , ý kiến của tôi là một ngôn ngữ mới giống như mở cửa sổ cửa sổ trên thế giới .\n",
      "  - Beam-5: Theo quan điểm của tôi , việc học một ngôn ngữ mới giống như mở cửa sổ cửa sổ trên thế giới .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample Translations ---\\n\")\n",
    "for sent in sample_en:\n",
    "    out_g = translate(sent, num_beams=1)\n",
    "    out_b = translate(sent, num_beams=5)\n",
    "    print(f\">> {sent}\")\n",
    "    print(f\"  - Greedy: {out_g}\")\n",
    "    print(f\"  - Beam-5: {out_b}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:38:18.466281Z",
     "iopub.status.busy": "2025-05-27T01:38:18.465700Z",
     "iopub.status.idle": "2025-05-27T01:38:19.204444Z",
     "shell.execute_reply": "2025-05-27T01:38:19.203682Z",
     "shell.execute_reply.started": "2025-05-27T01:38:18.466259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model và tokenizer đã được lưu tại: /kaggle/working/gpt_fs_saved_model\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Save model & tokenizer for demo/report\n",
    "SAVE_DIR = '/kaggle/working/gpt_fs_saved_model'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Lưu weights & config\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "\n",
    "# 2. Lưu tokenizer (vocab & merges)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"✅ Model và tokenizer đã được lưu tại: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:38:47.923725Z",
     "iopub.status.busy": "2025-05-27T01:38:47.923450Z",
     "iopub.status.idle": "2025-05-27T01:39:04.874716Z",
     "shell.execute_reply": "2025-05-27T01:39:04.874000Z",
     "shell.execute_reply.started": "2025-05-27T01:38:47.923697Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/gpt_fs_saved_model/ (stored 0%)\n",
      "  adding: kaggle/working/gpt_fs_saved_model/model.safetensors"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (deflated 7%)\n",
      "  adding: kaggle/working/gpt_fs_saved_model/special_tokens_map.json (deflated 45%)\n",
      "  adding: kaggle/working/gpt_fs_saved_model/generation_config.json (deflated 24%)\n",
      "  adding: kaggle/working/gpt_fs_saved_model/tokenizer_config.json (deflated 76%)\n",
      "  adding: kaggle/working/gpt_fs_saved_model/config.json (deflated 51%)\n",
      "  adding: kaggle/working/gpt_fs_saved_model/tokenizer.json (deflated 82%)\n",
      "✅ Đã tạo file: /kaggle/working/gpt_fs_saved_model.zip\n"
     ]
    }
   ],
   "source": [
    "# Cell X+1: Zip the saved model\n",
    "!zip -r /kaggle/working/gpt_fs_saved_model.zip /kaggle/working/gpt_fs_saved_model\n",
    "print(\"✅ Đã tạo file: /kaggle/working/gpt_fs_saved_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train tiếp từ 6 - 10 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T02:07:35.827791Z",
     "iopub.status.busy": "2025-05-27T02:07:35.827491Z",
     "iopub.status.idle": "2025-05-27T02:07:35.831404Z",
     "shell.execute_reply": "2025-05-27T02:07:35.830753Z",
     "shell.execute_reply.started": "2025-05-27T02:07:35.827772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "START_EPOCH = 6\n",
    "END_EPOCH   = 10\n",
    "PATIENCE    = 3\n",
    "CHECKPOINT_DIR = '/kaggle/working/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T02:04:49.249674Z",
     "iopub.status.busy": "2025-05-27T02:04:49.249072Z",
     "iopub.status.idle": "2025-05-27T02:04:49.521982Z",
     "shell.execute_reply": "2025-05-27T02:04:49.521206Z",
     "shell.execute_reply.started": "2025-05-27T02:04:49.249652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(\"/kaggle/input/model-cpkt\",'best_model.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T02:05:09.571639Z",
     "iopub.status.busy": "2025-05-27T02:05:09.570807Z",
     "iopub.status.idle": "2025-05-27T02:05:09.577773Z",
     "shell.execute_reply": "2025-05-27T02:05:09.577047Z",
     "shell.execute_reply.started": "2025-05-27T02:05:09.571609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = (len(train_loader)//ACCUMULATION_STEPS) * (END_EPOCH - START_EPOCH + 1)\n",
    "scheduler   = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T02:06:29.701622Z",
     "iopub.status.busy": "2025-05-27T02:06:29.700812Z",
     "iopub.status.idle": "2025-05-27T02:06:30.375113Z",
     "shell.execute_reply": "2025-05-27T02:06:30.374587Z",
     "shell.execute_reply.started": "2025-05-27T02:06:29.701596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bleu_metric  = evaluate.load('sacrebleu')\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "best_val_bleu = 0.0\n",
    "stale = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T02:50:35.892280Z",
     "iopub.status.busy": "2025-05-27T02:50:35.892014Z",
     "iopub.status.idle": "2025-05-27T02:50:35.895722Z",
     "shell.execute_reply": "2025-05-27T02:50:35.894956Z",
     "shell.execute_reply.started": "2025-05-27T02:50:35.892260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, math, torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T04:23:07.590471Z",
     "iopub.status.busy": "2025-05-27T04:23:07.589976Z",
     "iopub.status.idle": "2025-05-27T04:23:08.379401Z",
     "shell.execute_reply": "2025-05-27T04:23:08.378621Z",
     "shell.execute_reply.started": "2025-05-27T04:23:07.590447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_ckpt = os.path.join(CHECKPOINT_DIR, 'best_model.pt')\n",
    "torch.save(model.state_dict(), best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T02:58:05.850401Z",
     "iopub.status.busy": "2025-05-27T02:58:05.850115Z",
     "iopub.status.idle": "2025-05-27T02:58:06.324818Z",
     "shell.execute_reply": "2025-05-27T02:58:06.324010Z",
     "shell.execute_reply.started": "2025-05-27T02:58:05.850381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_ckpt = os.path.join(CHECKPOINT_DIR, 'best_model.pt')\n",
    "torch.save(model.state_dict(), best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T04:28:55.597638Z",
     "iopub.status.busy": "2025-05-27T04:28:55.597015Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb27b33c7384d1eafdc8b87980e30e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train E7:   0%|          | 0/26422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss = 1.7582\n",
      "  → Saved checkpoint /kaggle/working/checkpoints/epoch07.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc362b9844841a9bb82fb67945ad429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train E8:   0%|          | 0/26422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss = 1.5061\n",
      "  → Saved checkpoint /kaggle/working/checkpoints/epoch08.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd3fc44b9954d898bef45ffbefb3a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train E9:   0%|          | 0/26422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "START_EPOCH = 7\n",
    "END_EPOCH   = 10\n",
    "\n",
    "for epoch in range(START_EPOCH, END_EPOCH+1):\n",
    "    model.train(); total_loss = 0.; optimizer.zero_grad()\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Train E{epoch}\")):\n",
    "        out = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            labels=batch['labels'].to(device)\n",
    "        )\n",
    "        loss = out.loss / ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        if (step+1) % ACCUMULATION_STEPS == 0:\n",
    "            clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "            optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
    "    avg_train = total_loss * ACCUMULATION_STEPS / len(train_loader)\n",
    "\n",
    "    # (bạn có thể thêm validation/metrics ở đây nếu muốn)\n",
    "    print(f\"[Epoch {epoch}] Train Loss = {avg_train:.4f}\")\n",
    "\n",
    "    # Lưu checkpoint mỗi epoch\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, f'epoch{epoch:02d}.pt')\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    print(f\"  → Saved checkpoint {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T07:16:30.068283Z",
     "iopub.status.busy": "2025-05-27T07:16:30.068024Z",
     "iopub.status.idle": "2025-05-27T07:16:30.631137Z",
     "shell.execute_reply": "2025-05-27T07:16:30.630365Z",
     "shell.execute_reply.started": "2025-05-27T07:16:30.068265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model at: /kaggle/working/checkpoints/epoch09_draft.pt\n"
     ]
    }
   ],
   "source": [
    "# Chỉ cần model đang còn trong RAM:\n",
    "last_ckpt_path = os.path.join(CHECKPOINT_DIR, \"epoch09_draft.pt\")\n",
    "torch.save(model.state_dict(), last_ckpt_path)\n",
    "print(\"✅ Saved model at:\", last_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T07:19:42.153090Z",
     "iopub.status.busy": "2025-05-27T07:19:42.152423Z",
     "iopub.status.idle": "2025-05-27T07:19:52.658090Z",
     "shell.execute_reply": "2025-05-27T07:19:52.656937Z",
     "shell.execute_reply.started": "2025-05-27T07:19:42.153062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> I want to go to sleep\n",
      "-> Tôi muốn đi ngủ .\n",
      "\n",
      ">> Good morning! Did you sleep well?\n",
      "-> Chào buổi sáng bạn ngủ ngon .\n",
      "\n",
      ">> Yesterday the stock market plunged almost five percent.\n",
      "-> Ngày hôm qua thị trường chứng khoán đã giảm gần 5 %\n",
      "\n",
      ">> She wonders whether artificial intelligence will ever surpass human creativity.\n",
      "-> Bà cho rằng trí thông minh nhân tạo sẽ vượt qua sự sáng tạo của con người .\n",
      "\n",
      ">> Please, turn off the lights before you leave the laboratory.\n",
      "-> Hãy tắt đèn trước khi bạn rời phòng thí nghiệm\n",
      "\n",
      ">> Although it rained heavily, the concert continued until midnight.\n",
      "-> Mặc dù trong suốt buổi hoà nhạc vẫn còn nhiều hơn nữa cho đến nửa đêm .\n",
      "\n",
      ">> The report, which was published in 2023, estimates that global CO₂ emissions hit 37.4 gigatons.\n",
      "-> Báo cáo này đã được xuất bản năm 2023 ước tính rằng khí thải khí thải từ khí thải toàn cầu đã đạt tới 37.4,4,000 tấn .\n",
      "\n",
      ">> Have you ever tried Vietnamese egg coffee?\n",
      "-> Bạn đã bao giờ thử lấy cà phê trứng\n",
      "\n",
      ">> If we fail to act now, future generations will pay the price.\n",
      "-> Nếu chúng ta thất bại trong việc hành động tương lai sẽ phải trả giá .\n",
      "\n",
      ">> NASA's James Webb telescope recently captured breathtaking images of distant galaxies.\n",
      "-> NASA , James Webb gần đây đã chụp những hình ảnh ngoạn mục của những thiên hà xa xôi .\n",
      "\n",
      ">> In my opinion, learning a new language is like opening an extra window on the world.\n",
      "-> Theo tôi , việc học một ngôn ngữ mới giống như mở một cửa sổ cửa sổ trên thế giới .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_cpu(text, num_beams=5, max_new_tokens=64):\n",
    "    prompt = f\"Translate English to Vietnamese: {text} {tokenizer.eos_token}\"\n",
    "    enc = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=MAX_LENGTH//2)\n",
    "    input_ids = enc.input_ids\n",
    "    out = model_cpu.generate(\n",
    "        input_ids,\n",
    "        num_beams=num_beams,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(out[0, input_ids.size(-1):], skip_special_tokens=True)\n",
    "\n",
    "# Dịch thử\n",
    "for s in sample_en:\n",
    "    print(f\">> {s}\\n-> {translate_cpu(s)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7504992,
     "sourceId": 11937288,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7522590,
     "sourceId": 11963296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
